#!/usr/bin/env python3
"""
MUSE-Newsæ•°æ®é›†æ‰©å±•å·¥å…·
è°ƒç”¨å„ç§æ¨¡å‹APIå°†MUSE-Newsæ•°æ®é›†ä¸­çš„é—®é¢˜æ”¹å†™ä¸ºç­‰ä»·çš„å¤šä¸ªé—®é¢˜

ç”¨æ³•:
python extend_muse.py --model openai --output extended_muse.json
python extend_muse.py --model azure --azure-endpoint <endpoint> --output extended_muse.json
python extend_muse.py --model bedrock --aws-region us-east-1 --output extended_muse.json

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-01-31
"""

import argparse
import json
import logging
import os
import sys
import time
from abc import ABC, abstractmethod
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥data_loader
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from data_loader import MUSENewsDataLoader, Question
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥data_loader: {e}")
    print("è¯·ç¡®ä¿åœ¨src/ragç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# å¯é€‰çš„APIå®¢æˆ·ç«¯å¯¼å…¥
try:
    import openai
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

try:
    import google.generativeai as genai
    HAS_GOOGLE = True
except ImportError:
    HAS_GOOGLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ExtendedQuestion:
    """æ‰©å±•åçš„é—®é¢˜æ•°æ®ç»“æ„"""
    original_id: str
    original_question: str
    original_answer: List[str]
    rewritten_questions: List[str]
    timestamp: str
    model_used: str

class LLMExtender:
    """åŸºäºç°æœ‰LLMé€‚é…å™¨çš„æ‰©å±•å™¨ï¼Œä¸muse_rag_system.pyä¿æŒä¸€è‡´"""
    
    # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼Œä¸llm_adapter.pyä¿æŒä¸€è‡´
    SUPPORTED_MODELS = {
        'gpt-4o': {'provider': 'openai', 'api_key_env': 'OPENAI_API_KEY'},
        'gpt-4o-mini': {'provider': 'openai', 'api_key_env': 'OPENAI_API_KEY'}, 
        'gpt-4.5': {'provider': 'openai', 'api_key_env': 'OPENAI_API_KEY'},
        'gpt-O3': {'provider': 'openai', 'api_key_env': 'OPENAI_API_KEY'},
        'gpt-O3-mini': {'provider': 'openai', 'api_key_env': 'OPENAI_API_KEY'},
        'gpt-O4-mini': {'provider': 'openai', 'api_key_env': 'OPENAI_API_KEY'},
        'claude-3.5-sonnet': {'provider': 'anthropic', 'api_key_env': 'ANTHROPIC_API_KEY'},
        'claude-4-sonnet': {'provider': 'anthropic', 'api_key_env': 'ANTHROPIC_API_KEY'},
        'gemini-flash-2.5': {'provider': 'google', 'api_key_env': 'GOOGLE_API_KEY'},
        'gemini-pro-2.5': {'provider': 'google', 'api_key_env': 'GOOGLE_API_KEY'},
    }
    
    def __init__(self, model_name: str, temperature: float = 0.7, max_tokens: int = 1500):
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.client = None
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
        if model_name not in self.SUPPORTED_MODELS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(self.SUPPORTED_MODELS.keys())}")
        
        model_info = self.SUPPORTED_MODELS[model_name]
        self.provider = model_info['provider']
        
        # è·å–APIå¯†é’¥
        api_key = os.getenv(model_info['api_key_env'])
        if not api_key:
            raise ValueError(f"æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ {model_info['api_key_env']}")
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client(api_key)
        logger.info(f"åˆå§‹åŒ– {self.provider} APIï¼Œæ¨¡å‹: {model_name}")
    
    def _init_client(self, api_key: str):
        """åˆå§‹åŒ–ç›¸åº”çš„APIå®¢æˆ·ç«¯"""
        try:
            if self.provider == 'openai':
                if not HAS_OPENAI:
                    raise ImportError("è¯·å®‰è£…openaiåŒ…: pip install openai")
                self.client = openai.OpenAI(api_key=api_key)
            elif self.provider == 'anthropic':
                if not HAS_ANTHROPIC:
                    raise ImportError("è¯·å®‰è£…anthropicåŒ…: pip install anthropic")
                self.client = anthropic.Anthropic(api_key=api_key)
            elif self.provider == 'google':
                if not HAS_GOOGLE:
                    raise ImportError("è¯·å®‰è£…google-generativeaiåŒ…: pip install google-generativeai")
                genai.configure(api_key=api_key)
                self.client = genai.GenerativeModel(self._get_google_model_name())
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– {self.provider} å®¢æˆ·ç«¯å¤±è´¥: {e}")
            raise
    
    def _get_google_model_name(self) -> str:
        """è·å–Googleæ¨¡å‹çš„å®é™…åç§°"""
        google_model_mapping = {
            'gemini-flash-2.5': 'gemini-2.5-flash',  # Gemini 2.5 Flash (æ­£å¼ç‰ˆ)
            'gemini-pro-2.5': 'gemini-2.5-pro'       # Gemini 2.5 Pro (æ­£å¼ç‰ˆ)
        }
        return google_model_mapping.get(self.model_name, self.model_name)
    
    def create_rewrite_prompt(self, question: str, answer: str) -> str:
        """åˆ›å»ºé—®é¢˜æ”¹å†™çš„prompt"""
        return f"""Your task is to rewrite the given question into 5 different but semantically equivalent questions. The rewritten questions should:

1. Have the same answer as the original question
2. Use different wording and sentence structures
3. Maintain the same level of difficulty
4. Cover the same factual information
5. Be natural and well-formed

Original Question: {question}
Expected Answer: {answer if isinstance(answer, str) else ' / '.join(answer)}

Please provide exactly 5 rewritten questions, one per line, numbered 1-5:

1."""
    
    def generate_text(self, prompt: str) -> str:
        """ç»Ÿä¸€çš„æ–‡æœ¬ç”Ÿæˆæ¥å£"""
        messages = [
            {"role": "system", "content": "You are a helpful assistant that rewrites questions while preserving their meaning and expected answers."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            if self.provider == 'openai':
                return self._generate_openai(messages)
            elif self.provider == 'anthropic':
                return self._generate_anthropic(messages)
            elif self.provider == 'google':
                return self._generate_google(messages)
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ–‡æœ¬å¤±è´¥: {e}")
            raise
    
    def _generate_openai(self, messages: List[Dict[str, str]]) -> str:
        """OpenAI APIè°ƒç”¨"""
        # OpenAIæ¨¡å‹åç§°æ˜ å°„
        openai_model_mapping = {
            'gpt-4.5': 'gpt-4.5-preview',  # GPT-4.5æ­£å¼ç‰ˆAPIåç§°
        }
        
        model_name = openai_model_mapping.get(self.model_name, self.model_name)
        
        response = self.client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        return response.choices[0].message.content.strip()
    
    def _generate_anthropic(self, messages: List[Dict[str, str]]) -> str:
        """Anthropic APIè°ƒç”¨"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        system_message = ""
        claude_messages = []
        
        for msg in messages:
            if msg['role'] == 'system':
                system_message = msg['content']
            else:
                claude_messages.append({
                    'role': msg['role'],
                    'content': msg['content']
                })
        
        # Claudeæ¨¡å‹åç§°æ˜ å°„
        claude_model_mapping = {
            'claude-3.5-sonnet': 'claude-3-5-sonnet-20241022',
            'claude-4-sonnet': 'claude-sonnet-4-20250514'  # Claude 4 Sonnetæ­£å¼ç‰ˆ
        }
        
        model_name = claude_model_mapping.get(self.model_name, self.model_name)
        
        response = self.client.messages.create(
            model=model_name,
            system=system_message,
            messages=claude_messages,
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        return response.content[0].text
    
    def _generate_google(self, messages: List[Dict[str, str]]) -> str:
        """Google Gemini APIè°ƒç”¨"""
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
        prompt_parts = []
        for msg in messages:
            if msg['role'] == 'system':
                prompt_parts.append(f"System: {msg['content']}")
            elif msg['role'] == 'user':
                prompt_parts.append(f"User: {msg['content']}")
            elif msg['role'] == 'assistant':
                prompt_parts.append(f"Assistant: {msg['content']}")
        
        prompt = "\n\n".join(prompt_parts)
        
        # é…ç½®ç”Ÿæˆå‚æ•°
        generation_config = genai.types.GenerationConfig(
            temperature=self.temperature,
            max_output_tokens=self.max_tokens
        )
        
        response = self.client.generate_content(
            prompt,
            generation_config=generation_config
        )
        return response.text
    
    def parse_rewritten_questions(self, response: str) -> List[str]:
        """è§£ææ¨¡å‹å“åº”ï¼Œæå–æ”¹å†™çš„é—®é¢˜"""
        lines = response.strip().split('\n')
        questions = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å°è¯•åŒ¹é…ç¼–å·æ ¼å¼ (1. 2. 3. ç­‰)
            if line[0].isdigit() and ('.' in line or ')' in line):
                # ç§»é™¤ç¼–å·
                if '.' in line:
                    question = line.split('.', 1)[1].strip()
                elif ')' in line:
                    question = line.split(')', 1)[1].strip()
                else:
                    question = line
                
                if question and len(question) > 10:  # åŸºæœ¬è´¨é‡æ£€æŸ¥
                    questions.append(question)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¼–å·æ ¼å¼ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
        if len(questions) == 0:
            for line in lines:
                line = line.strip()
                if line and len(line) > 10 and '?' in line:
                    questions.append(line)
        
        return questions[:5]  # æœ€å¤šè¿”å›5ä¸ª

class MUSEExtender:
    """MUSE-Newsæ•°æ®é›†æ‰©å±•å™¨"""
    
    def __init__(self, llm_extender: LLMExtender, output_file: str):
        self.llm_extender = llm_extender
        self.output_file = output_file
        self.data_loader = MUSENewsDataLoader()
        self.results = []
    
    def load_data(self) -> List[Question]:
        """åŠ è½½MUSE-Newsæ•°æ®"""
        logger.info("æ­£åœ¨åŠ è½½MUSE-Newsæ•°æ®é›†...")
        try:
            knowmem_questions, _ = self.data_loader.load_evaluation_data()
            logger.info(f"æˆåŠŸåŠ è½½ {len(knowmem_questions)} ä¸ªé—®é¢˜")
            return knowmem_questions
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            raise
    
    def extend_question(self, question: Question) -> ExtendedQuestion:
        """æ‰©å±•å•ä¸ªé—®é¢˜"""
        logger.info(f"æ­£åœ¨å¤„ç†é—®é¢˜: {question.id}")
        
        # å‡†å¤‡ç­”æ¡ˆæ–‡æœ¬
        answer_text = question.answer[0] if question.answer else "æœªçŸ¥"
        
        # åˆ›å»ºæ”¹å†™prompt
        prompt = self.llm_extender.create_rewrite_prompt(question.question, answer_text)
        
        try:
            # è°ƒç”¨æ¨¡å‹API
            response = self.llm_extender.generate_text(prompt)
            logger.debug(f"æ¨¡å‹å“åº”: {response}")
            
            # è§£ææ”¹å†™çš„é—®é¢˜
            rewritten_questions = self.llm_extender.parse_rewritten_questions(response)
            
            if len(rewritten_questions) < 3:
                logger.warning(f"é—®é¢˜ {question.id} åªç”Ÿæˆäº† {len(rewritten_questions)} ä¸ªæ”¹å†™é—®é¢˜")
            
            return ExtendedQuestion(
                original_id=question.id,
                original_question=question.question,
                original_answer=question.answer,
                rewritten_questions=rewritten_questions,
                timestamp=datetime.now().isoformat(),
                model_used=self.llm_extender.model_name
            )
            
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜ {question.id} å¤±è´¥: {e}")
            # è¿”å›ç©ºçš„æ”¹å†™ç»“æœ
            return ExtendedQuestion(
                original_id=question.id,
                original_question=question.question,
                original_answer=question.answer,
                rewritten_questions=[],
                timestamp=datetime.now().isoformat(),
                model_used=self.llm_extender.model_name
            )
    
    def extend_all_questions(self, max_questions: Optional[int] = None, delay: float = 1.0):
        """æ‰©å±•æ‰€æœ‰é—®é¢˜"""
        questions = self.load_data()
        
        if max_questions:
            questions = questions[:max_questions]
            logger.info(f"é™åˆ¶å¤„ç†å‰ {max_questions} ä¸ªé—®é¢˜")
        
        logger.info(f"å¼€å§‹å¤„ç† {len(questions)} ä¸ªé—®é¢˜...")
        
        for i, question in enumerate(questions, 1):
            logger.info(f"è¿›åº¦: {i}/{len(questions)}")
            
            try:
                extended_question = self.extend_question(question)
                self.results.append(extended_question)
                
                # å®šæœŸä¿å­˜ç»“æœ
                if i % 10 == 0:
                    self.save_results(intermediate=True)
                    logger.info(f"å·²ä¿å­˜ä¸­é—´ç»“æœï¼Œå®Œæˆ {i} ä¸ªé—®é¢˜")
                
                # APIè°ƒç”¨é—´éš”
                if delay > 0 and i < len(questions):
                    time.sleep(delay)
                    
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} ä¸ªé—®é¢˜æ—¶å‡ºé”™: {e}")
                continue
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_results()
        logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œå…±æ‰©å±• {len(self.results)} ä¸ªé—®é¢˜")
    
    def save_results(self, intermediate: bool = False):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        output_file = self.output_file
        if intermediate:
            base, ext = os.path.splitext(self.output_file)
            output_file = f"{base}_intermediate{ext}"
        
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            data = {
                "metadata": {
                    "total_questions": len(self.results),
                    "model_used": self.llm_extender.model_name,
                    "generated_at": datetime.now().isoformat(),
                    "version": "1.0"
                },
                "extended_questions": [asdict(result) for result in self.results]
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            if not intermediate:
                logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
                
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

def create_llm_extender(model_name: str, temperature: float = 0.7, max_tokens: int = 1500) -> LLMExtender:
    """åˆ›å»ºLLMæ‰©å±•å™¨å®ä¾‹"""
    return LLMExtender(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens
    )

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="MUSE-Newsæ•°æ®é›†æ‰©å±•å·¥å…·",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        choices=['gpt-4o', 'gpt-4o-mini', 'gpt-4.5', 'gpt-O3', 'gpt-O3-mini', 'gpt-O4-mini',
                 'claude-3.5-sonnet', 'claude-4-sonnet', 
                 'gemini-flash-2.5', 'gemini-pro-2.5'],
        help="è¦ä½¿ç”¨çš„LLMæ¨¡å‹åç§°"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="extended_muse.json",
        help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: extended_muse.json)"
    )
    
    parser.add_argument(
        "--max-questions",
        type=int,
        help="æœ€å¤§å¤„ç†é—®é¢˜æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    
    parser.add_argument(
        "--delay",
        type=float,
        default=1.0,
        help="APIè°ƒç”¨é—´éš”ç§’æ•° (é»˜è®¤: 1.0)"
    )
    
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="LLMç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.7)"
    )
    
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=1500,
        help="LLMæœ€å¤§ç”Ÿæˆtokenæ•° (é»˜è®¤: 1500)"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # åˆ›å»ºLLMæ‰©å±•å™¨
        logger.info(f"åˆå§‹åŒ–LLMæ‰©å±•å™¨ï¼Œæ¨¡å‹: {args.model}...")
        llm_extender = create_llm_extender(
            model_name=args.model,
            temperature=args.temperature,
            max_tokens=args.max_tokens
        )
        
        # åˆ›å»ºæ‰©å±•å™¨
        extender = MUSEExtender(llm_extender, args.output)
        
        # å¼€å§‹æ‰©å±•
        extender.extend_all_questions(
            max_questions=args.max_questions,
            delay=args.delay
        )
        
        logger.info("ğŸ‰ æ•°æ®é›†æ‰©å±•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.info("âŒ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()