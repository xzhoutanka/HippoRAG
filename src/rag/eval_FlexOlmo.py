#!/usr/bin/env python3
"""
FlexOlmoæ¨¡å‹è¯„æµ‹è„šæœ¬
ä½¿ç”¨MUSE-Newsæ•°æ®é›†çš„knowmemåˆ†ç‰‡retain_qaæ•°æ®è¯„æµ‹FlexOlmoæ¨¡å‹å‡†ç¡®ç‡

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-01-31
"""

import argparse
import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple
from pathlib import Path

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# å¯¼å…¥é¡¹ç›®ä¸­çš„æ•°æ®åŠ è½½å™¨
from data_loader import MUSENewsDataLoader, Question

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FlexOlmoEvaluator:
    """FlexOlmoæ¨¡å‹è¯„æµ‹å™¨"""
    
    def __init__(self, model_path: str):
        """
        åˆå§‹åŒ–è¯„æµ‹å™¨
        
        Args:
            model_path: FlexOlmoæ¨¡å‹ç›®å½•è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # è¯„æµ‹ç»“æœç»Ÿè®¡
        self.results = {
            'total_questions': 0,
            'correct_answers': 0,
            'accuracy': 0.0,
            'detailed_results': [],
            'model_path': model_path,
            'timestamp': None,
            'evaluation_time': None
        }
        
    def load_model(self):
        """åŠ è½½FlexOlmoæ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"æ­£åœ¨åŠ è½½FlexOlmoæ¨¡å‹: {self.model_path}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        required_files = ['config.json', 'tokenizer.json']
        for file_name in required_files:
            file_path = os.path.join(self.model_path, file_name)
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file_path}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            logger.info("  åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            logger.info(f"  åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {self.device}")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            logger.info("âœ… FlexOlmoæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            self._print_model_info()
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½FlexOlmoæ¨¡å‹: {e}")
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        try:
            config_path = os.path.join(self.model_path, "config.json")
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            logger.info("=" * 50)
            logger.info("FlexOlmoæ¨¡å‹ä¿¡æ¯:")
            logger.info(f"  æ¨¡å‹ç±»å‹: {config.get('model_type', 'unknown')}")
            logger.info(f"  æ¶æ„: {config.get('architectures', ['unknown'])[0]}")
            logger.info(f"  éšè—å±‚å¤§å°: {config.get('hidden_size', 'unknown')}")
            logger.info(f"  å±‚æ•°: {config.get('num_hidden_layers', 'unknown')}")
            logger.info(f"  è¯æ±‡è¡¨å¤§å°: {config.get('vocab_size', 'unknown')}")
            
            # æ£€æŸ¥MoEä¿¡æ¯
            if 'num_experts' in config:
                logger.info(f"  ä¸“å®¶æ•°é‡: {config['num_experts']}")
                logger.info(f"  æ¯tokenæ¿€æ´»ä¸“å®¶æ•°: {config.get('num_experts_per_tok', 'unknown')}")
                logger.info("  âœ¨ è¿™æ˜¯ä¸€ä¸ªæ··åˆä¸“å®¶(MoE)æ¨¡å‹")
            
            logger.info("=" * 50)
            
        except Exception as e:
            logger.warning(f"è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    def generate_answer(self, question: str, max_length: int = 200) -> str:
        """
        ä½¿ç”¨FlexOlmoæ¨¡å‹ç”Ÿæˆé—®é¢˜ç­”æ¡ˆ
        
        Args:
            question: è¾“å…¥é—®é¢˜
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬
        """
        try:
            # æ„å»ºæç¤ºè¯ - ç”±äºFlexOlmoå·²ç»åŒ…å«Newsæ•°æ®ï¼Œç›´æ¥å›ç­”é—®é¢˜
            #prompt = f"Question: {question}\nAnswer:"
            prompt = f"""You are an expert news analyst tasked with answering questions based on factual knowledge from a news-related dataset. Your goal is to provide accurate, concise, and relevant answers to questions about news events, people, or topics. Follow these guidelines:

1. Answer only based on the factual knowledge you have been trained on.
2. If you are unsure or lack specific information, respond with "I don't have sufficient information to answer this question accurately."
3. Provide your answer in a clear, structured format: start with a direct response, followed by a brief explanation if necessary.
4. Avoid speculation, irrelevant details, or overly verbose responses.

**Question**: {question}

**Answer**:
"""
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            
            # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
            if self.device == "cuda":
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # ç”Ÿæˆç­”æ¡ˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + max_length,
                    temperature=0.1,  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ç­”æ¡ˆ
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_return_sequences=1
                )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç­”æ¡ˆéƒ¨åˆ†ï¼ˆå»é™¤é—®é¢˜éƒ¨åˆ†ï¼‰
            if "Answer:" in generated_text:
                answer = generated_text.split("Answer:")[-1].strip()
            else:
                answer = generated_text[len(prompt):].strip()
            
            # æ¸…ç†ç­”æ¡ˆæ–‡æœ¬
            answer = self._clean_answer(answer)
            
            return answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}")
            return ""
    
    def _clean_answer(self, answer: str) -> str:
        """æ¸…ç†ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        answer = answer.strip()
        
        # ç§»é™¤å¯èƒ½çš„é‡å¤å†…å®¹
        lines = answer.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in cleaned_lines:
                cleaned_lines.append(line)
        
        # å¦‚æœæœ‰å¤šè¡Œï¼Œåªå–ç¬¬ä¸€è¡Œä½œä¸ºç­”æ¡ˆ
        if cleaned_lines:
            answer = cleaned_lines[0]
        
        # é™åˆ¶ç­”æ¡ˆé•¿åº¦
        if len(answer) > 500:
            answer = answer[:500].strip()
        
        return answer
    
    def check_answer_correctness(self, generated_answer: str, correct_answers: List[str]) -> bool:
        """
        æ£€æŸ¥ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        
        Args:
            generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            correct_answers: æ­£ç¡®ç­”æ¡ˆåˆ—è¡¨
            
        Returns:
            æ˜¯å¦æ­£ç¡®
        """
        if not generated_answer or not correct_answers:
            return False
        
        generated_answer = generated_answer.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•æ­£ç¡®ç­”æ¡ˆ
        for correct_answer in correct_answers:
            if not correct_answer:
                continue
                
            correct_answer = correct_answer.lower().strip()
            
            # å®Œå…¨åŒ¹é…
            if generated_answer == correct_answer:
                return True
            
            # åŒ…å«åŒ¹é…
            if correct_answer in generated_answer or generated_answer in correct_answer:
                return True
            
            # åŸºäºå…³é”®è¯çš„æ¨¡ç³ŠåŒ¹é…
            if self._fuzzy_match(generated_answer, correct_answer):
                return True
        
        return False
    
    def _fuzzy_match(self, generated: str, correct: str) -> bool:
        """æ¨¡ç³ŠåŒ¹é…ç­”æ¡ˆ"""
        # æå–å…³é”®è¯
        import re
        generated_words = set(re.findall(r'\b\w+\b', generated.lower()))
        correct_words = set(re.findall(r'\b\w+\b', correct.lower()))
        
        # è®¡ç®—äº¤é›†æ¯”ä¾‹
        if not correct_words:
            return False
            
        intersection = generated_words.intersection(correct_words)
        similarity = len(intersection) / len(correct_words)
        
        # å¦‚æœå…³é”®è¯é‡å è¶…è¿‡60%ï¼Œè®¤ä¸ºæ˜¯æ­£ç¡®çš„
        return similarity >= 0.6
    
    def evaluate_questions(self, questions: List[Question]) -> Dict[str, Any]:
        """
        è¯„æµ‹é—®é¢˜åˆ—è¡¨
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            
        Returns:
            è¯„æµ‹ç»“æœ
        """
        logger.info(f"å¼€å§‹è¯„æµ‹ {len(questions)} ä¸ªé—®é¢˜...")
        
        start_time = time.time()
        correct_count = 0
        
        for i, question in enumerate(questions):
            logger.info(f"å¤„ç†é—®é¢˜ {i+1}/{len(questions)}: {question.id}")
            
            try:
                # ç”Ÿæˆç­”æ¡ˆ
                generated_answer = self.generate_answer(question.question)
                
                # æ£€æŸ¥æ­£ç¡®æ€§
                is_correct = self.check_answer_correctness(generated_answer, question.answer)
                
                if is_correct:
                    correct_count += 1
                
                # è®°å½•è¯¦ç»†ç»“æœ
                result_detail = {
                    'question_id': question.id,
                    'question': question.question,
                    'correct_answers': question.answer,
                    'generated_answer': generated_answer,
                    'is_correct': is_correct
                }
                
                self.results['detailed_results'].append(result_detail)
                
                # æ‰“å°è¿›åº¦
                if (i + 1) % 10 == 0 or is_correct:
                    status = "âœ…" if is_correct else "âŒ"
                    logger.info(f"  {status} é—®é¢˜ {i+1}: {'æ­£ç¡®' if is_correct else 'é”™è¯¯'}")
                    logger.info(f"    é—®é¢˜: {question.question}")
                    logger.info(f"    ç”Ÿæˆç­”æ¡ˆ: {generated_answer}")
                    logger.info(f"    æ­£ç¡®ç­”æ¡ˆ: {question.answer}")
                
            except Exception as e:
                logger.error(f"å¤„ç†é—®é¢˜ {question.id} æ—¶å‡ºé”™: {e}")
                # è®°å½•é”™è¯¯ç»“æœ
                result_detail = {
                    'question_id': question.id,
                    'question': question.question,
                    'correct_answers': question.answer,
                    'generated_answer': f"é”™è¯¯: {str(e)}",
                    'is_correct': False
                }
                self.results['detailed_results'].append(result_detail)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        end_time = time.time()
        evaluation_time = end_time - start_time
        
        self.results.update({
            'total_questions': len(questions),
            'correct_answers': correct_count,
            'accuracy': correct_count / len(questions) if questions else 0.0,
            'timestamp': datetime.now().isoformat(),
            'evaluation_time': evaluation_time
        })
        
        return self.results
    
    def save_results(self, output_file: str = None):
        """ä¿å­˜è¯„æµ‹ç»“æœ"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"flexolmo_evaluation_results_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def print_summary(self):
        """æ‰“å°è¯„æµ‹æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("FlexOlmoæ¨¡å‹è¯„æµ‹ç»“æœæ‘˜è¦")
        print("=" * 60)
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"è¯„æµ‹æ—¶é—´: {self.results.get('timestamp', 'unknown')}")
        print(f"æ€»é—®é¢˜æ•°: {self.results['total_questions']}")
        print(f"æ­£ç¡®å›ç­”: {self.results['correct_answers']}")
        print(f"å‡†ç¡®ç‡: {self.results['accuracy']:.3f} ({self.results['accuracy']*100:.1f}%)")
        print(f"è¯„æµ‹è€—æ—¶: {self.results.get('evaluation_time', 0):.1f} ç§’")
        print("=" * 60)
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹ç»“æœ
        if self.results['detailed_results']:
            print("\nç¤ºä¾‹ç»“æœ:")
            for i, result in enumerate(self.results['detailed_results'][:3]):
                status = "âœ…" if result['is_correct'] else "âŒ"
                print(f"\n{status} ç¤ºä¾‹ {i+1}:")
                print(f"  é—®é¢˜: {result['question']}")
                print(f"  ç”Ÿæˆç­”æ¡ˆ: {result['generated_answer']}")
                print(f"  æ­£ç¡®ç­”æ¡ˆ: {result['correct_answers']}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="FlexOlmoæ¨¡å‹è¯„æµ‹è„šæœ¬",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        "model_path",
        type=str,
        help="FlexOlmoæ¨¡å‹ç›®å½•è·¯å¾„"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¯„æµ‹ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)"
    )
    
    parser.add_argument(
        "--max_length",
        type=int,
        default=200,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦ (é»˜è®¤: 200)"
    )
    
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶è¯„æµ‹é—®é¢˜æ•°é‡ (é»˜è®¤: å…¨éƒ¨)"
    )
    
    args = parser.parse_args()
    
    try:
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(args.model_path):
            logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            sys.exit(1)
        
        # åˆ›å»ºè¯„æµ‹å™¨
        logger.info("åˆå§‹åŒ–FlexOlmoè¯„æµ‹å™¨...")
        evaluator = FlexOlmoEvaluator(args.model_path)
        
        # åŠ è½½æ¨¡å‹
        evaluator.load_model()
        
        # åŠ è½½è¯„æµ‹æ•°æ®
        logger.info("åŠ è½½MUSE-Newsæ•°æ®é›†...")
        data_loader = MUSENewsDataLoader()
        
        # åªåŠ è½½knowmemåˆ†ç‰‡çš„retain_qaæ•°æ®
        knowmem_questions, _ = data_loader.load_evaluation_data()
        
        if not knowmem_questions:
            logger.error("âŒ æ²¡æœ‰åŠ è½½åˆ°knowmemçš„retain_qaæ•°æ®")
            sys.exit(1)
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(knowmem_questions)} ä¸ªretain_qaé—®é¢˜")
        
        # é™åˆ¶é—®é¢˜æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.limit and args.limit < len(knowmem_questions):
            knowmem_questions = knowmem_questions[:args.limit]
            logger.info(f"é™åˆ¶è¯„æµ‹é—®é¢˜æ•°é‡ä¸º: {args.limit}")
        
        # å¼€å§‹è¯„æµ‹
        logger.info("å¼€å§‹FlexOlmoæ¨¡å‹è¯„æµ‹...")
        results = evaluator.evaluate_questions(knowmem_questions)
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_summary()
        
        # ä¿å­˜ç»“æœ
        evaluator.save_results(args.output)
        
        logger.info("ğŸ‰ è¯„æµ‹å®Œæˆ!")
        
    except KeyboardInterrupt:
        logger.info("âŒ ç”¨æˆ·ä¸­æ–­è¯„æµ‹")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
